{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4086b59-5e7e-4398-9038-61d925062876",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install gradio>=4.36.0 pandas reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9a5449-bfd8-481b-a6f8-abb7196ba866",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, io, json, traceback, datetime, re\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# --- PDF helpers (ReportLab) ---\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, Preformatted\n",
    "\n",
    "from xml.sax.saxutils import escape  # for safe Paragraph content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37db8f8c-c895-4ad0-8378-69a1817ff413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generation\n",
    "def generate_candidate_answer(task_prompt: str, *, candidate_model: str = \"gpt-4o-mini\") -> str:\n",
    "\n",
    "    # Generate a sample response to prompt with the model-under-test.\n",
    "\n",
    "    vs = globals().get(\"vs_id\")\n",
    "\n",
    "    # Build tools only when a vector store is available\n",
    "    tools = [{\"type\": \"file_search\", \"vector_store_ids\": [vs]}] if vs else None\n",
    "\n",
    "    # Only include 'tools' in the API call if we actually built it\n",
    "    extra_kwargs = {\"tools\": tools} if tools else {}\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=candidate_model,\n",
    "        input=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Follow the instructions carefully. Use attached files when noted. Include any code, values, files, or plots the task requires.\\n\\n\"\n",
    "                f\"TASK:\\n{task_prompt}\"\n",
    "            )\n",
    "        }],\n",
    "        **({\"tools\": tools} if tools else {})\n",
    "        #tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}],\n",
    "        # temperature=0\n",
    "        #**extra_kwargs\n",
    "    )\n",
    "    # SDK compatibility: try the modern accessor, fall back if needed.\n",
    "    try:\n",
    "        return resp.output_text\n",
    "    except AttributeError:\n",
    "        return resp.output[0].content[0].text\n",
    "\n",
    "# Rubric preparation (ONLY criterion + score)\n",
    "def _prepare_rubric_items(raw_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Prepare rubric items for grading. IMPORTANT: penalty is determined ONLY by the sign of `score`.\n",
    "      - score > 0  => reward item\n",
    "      - score < 0  => penalty item\n",
    "    \"\"\"\n",
    "    prepped = []\n",
    "    for it in raw_items:\n",
    "        crit  = (it.get(\"criterion\") or \"\").strip()\n",
    "        score = float(it.get(\"score\", 0))\n",
    "        rid   = it.get(\"rubricItemId\") or \"\"\n",
    "        is_penalty = (score < 0)  # <-- CHANGED: sign-of-score, not wording\n",
    "        prepped.append({\n",
    "            \"rubricItemId\": rid,\n",
    "            \"criterion\": crit,\n",
    "            \"score\": score,          # positive or negative; used as-is\n",
    "            \"is_penalty\": is_penalty\n",
    "        })\n",
    "    return prepped\n",
    "\n",
    "def _build_rubric_prompts(sample, completion, items):\n",
    "    # Only pass what the grader truly needs: id + criterion (hide score/is_penalty)\n",
    "    compact = [{\n",
    "        \"rubricItemId\": it[\"rubricItemId\"],\n",
    "        \"criterion\": it[\"criterion\"]\n",
    "    } for it in items]\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a meticulous grader that outputs STRICT JSON per schema.\\n\"\n",
    "        \"Evaluate each rubric item independently using ONLY its 'criterion' text. \"\n",
    "        \"Ignore all other rubric metadata (scores, penalties, tags, sources) and any external facts.\\n\"\n",
    "        \"Unit handling: If a numeric value in the candidate is within the stated tolerance after proper unit conversion, \"\n",
    "        \"consider the criterion satisfied even if the candidate does not restate the value in the rubric's unit.\\n\"\n",
    "        \"Consistency rule: The boolean 'met' must reflect the final verdict in your rationale.\\n\"\n",
    "    )\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "TASK PROMPT:\n",
    "{sample.get('prompt','')}\n",
    "\n",
    "CANDIDATE ANSWER:\n",
    "{completion}\n",
    "\n",
    "RUBRIC_ITEMS (evaluate ONLY against criterion; ignore any other metadata):\n",
    "{json.dumps(compact, ensure_ascii=False)}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Decide each item strictly from the candidate answer vs. the item's 'criterion'. Do NOT use outside knowledge.\n",
    "- Treat values as equivalent across unit systems if, after correct conversion, the value is within the stated tolerance.\n",
    "- Write a short rationale. The last sentence MUST be exactly one of:\n",
    "  \"Verdict: MET\"  or  \"Verdict: NOT MET\".\n",
    "- Set the boolean 'met' to True iff the final sentence is \"Verdict: MET\"; else set it to False.\n",
    "- Never output a rationale that concludes \"Verdict: MET\" while setting met=false, or vice versa.\n",
    "- If a tolerance is written in a criteria and the value in the response is within the tolerance, then \"Verdict: MET\"\n",
    "\n",
    "OUTPUT (STRICT JSON ONLY):\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"rubricItemId\": \"string\",\n",
    "      \"met\": true | false,\n",
    "      \"rationale\": \"string that ends with 'Verdict: MET' or 'Verdict: NOT MET'\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "    return system_message, user_message\n",
    "\n",
    "\n",
    "def _rubric_bounds(items: List[Dict[str, Any]]) -> Tuple[float, float]:\n",
    "    pos = sum(s[\"score\"] for s in items if s[\"score\"] > 0)\n",
    "    neg = sum(s[\"score\"] for s in items if s[\"score\"] < 0)\n",
    "    return pos, neg  # (max positive, min negative)\n",
    "\n",
    "\n",
    "# Exact-match grader (optional)\n",
    "\n",
    "def simple_string_grader(sample, completion) -> float:\n",
    "\n",
    "    #1.0 if completion exactly matches sample['reference'], else 0.0.\n",
    "    #Leave sample['reference'] empty ('') to effectively ignore this.\n",
    "\n",
    "    ref = (sample.get(\"reference\") or \"\").strip()\n",
    "    if not ref:\n",
    "        return 0.0\n",
    "    return 1.0 if completion.strip() == ref else 0.0\n",
    "\n",
    "\n",
    "def model_grader_with_rubric(sample, completion, raw_rubric_items, *, grader_model: str = \"gpt-4o-mini\"):\n",
    "    items = _prepare_rubric_items(raw_rubric_items)\n",
    "    pos_max = sum(max(0.0, float(it[\"score\"])) for it in items)\n",
    "    neg_min = sum(min(0.0, float(it[\"score\"])) for it in items)\n",
    "    bounds = {\"pos_max\": float(pos_max), \"neg_min\": float(neg_min)}\n",
    "\n",
    "    system_message, user_message = _build_rubric_prompts(sample, completion, items)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=grader_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        #temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content\n",
    "\n",
    "    def _strip_fences(s: str) -> str:\n",
    "        s = s.strip()\n",
    "        if s.startswith(\"```\"):\n",
    "            parts = s.split(\"```\")\n",
    "            if len(parts) >= 3:\n",
    "                s = parts[1]\n",
    "            s = s.lstrip()\n",
    "            if s.startswith(\"json\"):\n",
    "                s = s[4:].lstrip()\n",
    "        return s\n",
    "\n",
    "    try:\n",
    "        rubric_json = json.loads(content)\n",
    "    except Exception:\n",
    "        rubric_json = json.loads(_strip_fences(content))\n",
    "\n",
    "    if not isinstance(rubric_json, dict) or \"items\" not in rubric_json:\n",
    "        raise ValueError(\"Grader did not return JSON with an 'items' array per schema.\")\n",
    "\n",
    "        # Enforce: boolean must match final \"Verdict: ...\" line in rationale\n",
    "    for it in rubric_json.get(\"items\", []):\n",
    "        tail = re.sub(r\"\\s+\", \" \", str(it.get(\"rationale\", \"\"))).strip().lower().rstrip(\" .!;\")\n",
    "    if tail.endswith(\"verdict: met\"):\n",
    "        it[\"met\"] = True\n",
    "    elif tail.endswith(\"verdict: not met\"):\n",
    "        it[\"met\"] = False\n",
    "\n",
    "    return rubric_json, bounds\n",
    "\n",
    "\n",
    "def _compute_signed_total_from_decisions(rubric_json: dict, raw_rubric_items):\n",
    "    \"\"\"\n",
    "    Compute signed total strictly from rubric item scores and decisions.\n",
    "    Also return enriched per-item rows including:\n",
    "      - criterion_number (1-indexed position in the original rubric JSON)\n",
    "      - point_value (the rubric score, positive or negative)\n",
    "      - awarded (0 or point_value, unless a valid 'awarded' was provided)\n",
    "\n",
    "    Rules:\n",
    "    - If 'met' is True -> contribution = rubric score (can be negative for penalties)\n",
    "    - If 'met' is False -> contribution = 0\n",
    "    - Only trust 'awarded' if it is exactly 0 or exactly equal to the rubric score\n",
    "    \"\"\"\n",
    "    # Build lookups from the rubric’s original order\n",
    "    id_to_score = {}\n",
    "    id_to_index1 = {}\n",
    "    id_to_criterion = {}\n",
    "    for idx, it in enumerate(raw_rubric_items, start=1):\n",
    "        rid = it.get(\"rubricItemId\") or it.get(\"id\") or (it.get(\"criterion\", \"\")[:48])\n",
    "        sc = float(it.get(\"score\", 0.0))\n",
    "        id_to_score[rid] = sc\n",
    "        id_to_index1[rid] = idx\n",
    "        id_to_criterion[rid] = it.get(\"criterion\", \"\")\n",
    "\n",
    "    # Pull decisions list from the grader output (be tolerant of formats)\n",
    "    if isinstance(rubric_json, dict):\n",
    "        decisions = rubric_json.get(\"items\") or rubric_json.get(\"results\") or []\n",
    "    elif isinstance(rubric_json, list):\n",
    "        decisions = rubric_json\n",
    "    else:\n",
    "        decisions = []\n",
    "\n",
    "    items_out = []\n",
    "    total = 0.0\n",
    "\n",
    "    for dec in decisions:\n",
    "        rid = dec.get(\"rubricItemId\") or dec.get(\"id\") or (dec.get(\"criterion\", \"\")[:48])\n",
    "        score = id_to_score.get(rid, 0.0)\n",
    "        met = bool(dec.get(\"met\", False))\n",
    "\n",
    "        # Only trust 'awarded' if it's exactly 0 or exactly equal to the rubric score\n",
    "        aw = dec.get(\"awarded\", None)\n",
    "        if isinstance(aw, (int, float)) and (abs(float(aw) - score) < 1e-9 or abs(float(aw)) < 1e-9):\n",
    "            awarded = float(aw)\n",
    "        else:\n",
    "            awarded = score if met else 0.0\n",
    "\n",
    "        total += awarded\n",
    "        items_out.append({\n",
    "            \"criterion_number\": id_to_index1.get(rid, None),\n",
    "            \"rubricItemId\": rid,\n",
    "            \"criterion\": id_to_criterion.get(rid, \"\"),\n",
    "            \"point_value\": score,\n",
    "            \"met\": met,\n",
    "            \"awarded\": awarded,\n",
    "            \"rationale\": dec.get(\"rationale\", \"\")\n",
    "        })\n",
    "\n",
    "    return total, items_out\n",
    "\n",
    "# Normalize rubric total to [0,1] and blend with exact-match\n",
    "def _normalize_total_score(total: float, pos_max: float, neg_min: float) -> float:\n",
    "    \"\"\"\n",
    "    Map a signed total into [0,1] given bounds:\n",
    "      neg_min <= total <= pos_max\n",
    "    \"\"\"\n",
    "    lo = float(neg_min)\n",
    "    hi = float(pos_max)\n",
    "    span = hi - lo\n",
    "    if span == 0:\n",
    "        return 0.5\n",
    "    norm = (float(total) - lo) / span\n",
    "    # Clamp for safety\n",
    "    return max(0.0, min(1.0, norm))\n",
    "\n",
    "\n",
    "def multigrader(sample, completion, raw_rubric_items,\n",
    "                *, grader_model: str = \"gpt-4o-mini\",\n",
    "                exact_weight: float = 0.0, rubric_weight: float = 1.0):\n",
    "    \"\"\"\n",
    "    Returns (final_score_0_1, details).\n",
    "    This version recomputes score from decisions.\n",
    "    - Penalties/rewards are determined solely by the sign of each rubric item's score.\n",
    "    - 'rubric_details' returns enriched per-item rows (criterion number, point value, rationale, etc.).\n",
    "    \"\"\"\n",
    "    # 1) Exact-match channel (if you use it)\n",
    "    exact = simple_string_grader(sample, completion)\n",
    "\n",
    "    # 2) Ask the grader model for per-item decisions and bounds\n",
    "    #    Expect: rubric_json = {\"items\":[...]} and bounds = {\"pos_max\": float, \"neg_min\": float}\n",
    "    rubric_json, bounds = model_grader_with_rubric(\n",
    "        sample, completion, raw_rubric_items, grader_model=grader_model\n",
    "    )\n",
    "\n",
    "    # 3) Recompute signed total from decisions (now returns (signed_total, items_out))\n",
    "    signed_total, items_out = _compute_signed_total_from_decisions(rubric_json, raw_rubric_items)\n",
    "\n",
    "    # 4) Normalize and blend\n",
    "    rubric_norm = _normalize_total_score(signed_total, bounds.get(\"pos_max\", 0.0), bounds.get(\"neg_min\", 0.0))\n",
    "    final = (exact_weight * float(exact)) + (rubric_weight * float(rubric_norm))\n",
    "\n",
    "    # 5) Return details\n",
    "    return final, {\n",
    "        \"exact_match\": float(exact),\n",
    "        \"rubric_total\": float(signed_total),      # signed\n",
    "        \"rubric_norm\": float(rubric_norm),\n",
    "        \"rubric_bounds\": bounds,\n",
    "        \"rubric_details\": {\"items\": items_out},   # enriched rows for your table/JSON panel\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bd76d88-206a-4e32-b984-62119bb57b7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using as-is: deliverable.txt\n"
     ]
    }
   ],
   "source": [
    "# UPDATED GRADIO UI WITH REGRADE SUPPORT\n",
    "# ---------- Helpers ----------\n",
    "def _fpath(obj):\n",
    "    if obj is None:\n",
    "        return None\n",
    "    return getattr(obj, \"name\", None) or getattr(obj, \"path\", None) or str(obj)\n",
    "\n",
    "def _read_text_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def _read_json_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _ensure_client_from_key(api_key_text):\n",
    "    # Reuse existing client if present; otherwise create from textbox/env\n",
    "    if \"client\" in globals() and globals()[\"client\"] is not None:\n",
    "        return globals()[\"client\"]\n",
    "    api_key = (api_key_text or \"\").strip() or os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"No OpenAI client found and no API key provided.\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Please install openai>=1.40.0\") from e\n",
    "    globals()[\"client\"] = OpenAI(api_key=api_key)\n",
    "    return globals()[\"client\"]\n",
    "\n",
    "# File types we can upload directly to File Search (keep conservative)\n",
    "SUPPORTED_FOR_RETRIEVAL = {\n",
    "    \".pdf\", \".txt\", \".md\", \".rtf\", \".html\", \".htm\", \".docx\", \".pptx\"\n",
    "}\n",
    "\n",
    "# Types we'll transparently convert to .txt before upload\n",
    "CONVERT_TO_TXT = {\n",
    "    \".csv\", \".tsv\", \".py\", \".json\", \".yaml\", \".yml\", \".ipynb\"\n",
    "}\n",
    "\n",
    "def _stage_for_retrieval(paths, *, max_csv_rows=2000):\n",
    "    \"\"\"\n",
    "    Ensure every path is uploadable by File Search:\n",
    "      - If extension is supported => keep as is\n",
    "      - Else => convert to .txt (CSV/TSV truncated to first `max_csv_rows` rows)\n",
    "    Returns (staged_paths, notes) where staged_paths are Paths safe to upload.\n",
    "    \"\"\"\n",
    "    staging_dir = Path.cwd() / \"_staged_refs\"\n",
    "    staging_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    staged, notes = [], []\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        ext = p.suffix.lower()\n",
    "\n",
    "        if ext in SUPPORTED_FOR_RETRIEVAL:\n",
    "            staged.append(p)\n",
    "            notes.append(f\"Using as-is: {p.name}\")\n",
    "            continue\n",
    "\n",
    "        if ext in CONVERT_TO_TXT:\n",
    "            txt_name = f\"{p.stem}{ext.replace('.', '_')}.txt\"\n",
    "            txt_path = staging_dir / txt_name\n",
    "            try:\n",
    "                if ext in {\".csv\", \".tsv\"}:\n",
    "                    # Load limited rows; write back as CSV text (UTF-8)\n",
    "                    sep = \"\\t\" if ext == \".tsv\" else \",\"\n",
    "                    df = pd.read_csv(p, sep=sep, nrows=max_csv_rows, dtype=str, encoding=\"utf-8\", engine=\"python\")\n",
    "                    txt_path.write_text(df.to_csv(index=False), encoding=\"utf-8\")\n",
    "                    notes.append(f\"Converted {p.name} → {txt_path.name} (first {len(df)} rows).\")\n",
    "                else:\n",
    "                    # Treat as text-like: code, json, yaml, ipynb (raw)\n",
    "                    raw = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                    header = f\"### Original file: {p.name}\\n\\n\"\n",
    "                    txt_path.write_text(header + raw, encoding=\"utf-8\")\n",
    "                    notes.append(f\"Converted {p.name} → {txt_path.name}.\")\n",
    "                staged.append(txt_path)\n",
    "            except Exception as e:\n",
    "                notes.append(f\"Skipped {p.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Unknown/unsupported: try raw text fallback; if fails, skip\n",
    "        try:\n",
    "            txt_path = staging_dir / f\"{p.stem}{ext.replace('.', '_')}.txt\"\n",
    "            raw = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            txt_path.write_text(f\"### Original file: {p.name}\\n\\n\" + raw, encoding=\"utf-8\")\n",
    "            staged.append(txt_path)\n",
    "            notes.append(f\"Converted (fallback) {p.name} → {txt_path.name}.\")\n",
    "        except Exception as e:\n",
    "            notes.append(f\"Skipped {p.name}: unsupported and not convertible ({e}).\")\n",
    "\n",
    "    return staged, notes\n",
    "\n",
    "\n",
    "def _create_vs_and_upload_refs(client, ref_file_objs):\n",
    "    \"\"\"\n",
    "    Creates a vector store and uploads supporting refs.\n",
    "    Unsupported types (e.g., CSV, PY) are auto-converted to .txt first.\n",
    "    Sets global `vs_id` and `VS`.\n",
    "    \"\"\"\n",
    "    VS = getattr(client, \"vector_stores\", None) or client.beta.vector_stores\n",
    "    vs = VS.create(name=\"prompt_refs\")\n",
    "    vs_id = vs.id\n",
    "\n",
    "    # Collect local paths\n",
    "    ref_paths = []\n",
    "    for rf in (ref_file_objs or []):\n",
    "        p = _fpath(rf)\n",
    "        if p and Path(p).exists():\n",
    "            ref_paths.append(Path(p))\n",
    "\n",
    "    # Stage for retrieval (convert unsupported → .txt)\n",
    "    staged_paths, notes = _stage_for_retrieval(ref_paths)\n",
    "    # Make these notes visible in the UI log if `log` exists in outer scope\n",
    "    try:\n",
    "        for n in notes:\n",
    "            print(n)  # Gradio captures stdout; plus we append to logs in run_pipeline\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if staged_paths:\n",
    "        file_batches = getattr(VS, \"file_batches\", None)\n",
    "        if file_batches and hasattr(file_batches, \"upload_and_poll\"):\n",
    "            streams = [open(str(p), \"rb\") for p in staged_paths]\n",
    "            try:\n",
    "                file_batches.upload_and_poll(vector_store_id=vs_id, files=streams)\n",
    "            finally:\n",
    "                for s in streams:\n",
    "                    try: s.close()\n",
    "                    except: pass\n",
    "        else:\n",
    "            for p in staged_paths:\n",
    "                f = client.files.create(file=open(str(p), \"rb\"), purpose=\"assistants\")\n",
    "                VS.files.create(vector_store_id=vs_id, file_id=f.id)\n",
    "\n",
    "    globals()[\"vs_id\"] = vs_id\n",
    "    globals()[\"VS\"] = VS\n",
    "    return vs_id\n",
    "\n",
    "def _build_items_table(items_raw):\n",
    "    \"\"\"\n",
    "    Normalize per-item rows into a tidy DataFrame:\n",
    "    ['#','criterion','score','rationale','verdict','awarded']\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for idx, it in enumerate(items_raw or [], start=1):\n",
    "        criterion_number = it.get(\"criterion_number\", idx)\n",
    "        criterion = it.get(\"criterion\") or it.get(\"rubricItemId\") or \"\"\n",
    "        point_value = it.get(\"point_value\", it.get(\"score\", 0.0))\n",
    "        try: point_value = float(point_value)\n",
    "        except: point_value = 0.0\n",
    "        met = bool(it.get(\"met\", False))\n",
    "        verdict = \"Met\" if met else \"Not Met\"\n",
    "        awarded = it.get(\"awarded\", point_value if met else 0.0)\n",
    "        try: awarded = float(awarded)\n",
    "        except: awarded = 0.0\n",
    "        rationale = it.get(\"rationale\", \"\")\n",
    "        rows.append({\n",
    "            \"#\": criterion_number,\n",
    "            \"criterion\": criterion,\n",
    "            \"score\": point_value,\n",
    "            \"rationale\": rationale,\n",
    "            \"verdict\": verdict,\n",
    "            \"awarded\": awarded,\n",
    "        })\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"#\", \"criterion\", \"score\", \"rationale\", \"verdict\", \"awarded\"])\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df[\"#\"].notna().any():\n",
    "        df = df.sort_values(by=[\"#\"], kind=\"stable\")\n",
    "    return df[[\"#\", \"criterion\", \"score\", \"rationale\", \"verdict\", \"awarded\"]]\n",
    "\n",
    "def _out_dir():\n",
    "    \"\"\"\n",
    "    Cross-platform output dir:\n",
    "      1) env GRADING_REPORT_DIR if set\n",
    "      2) ~/Downloads if it exists\n",
    "      3) ./grading_reports\n",
    "    \"\"\"\n",
    "    env = os.getenv(\"GRADING_REPORT_DIR\", \"\")\n",
    "    if env:\n",
    "        d = Path(env).expanduser()\n",
    "    else:\n",
    "        candidates = [Path.home()/ \"Downloads\", Path.home()/ \"OneDrive\"/ \"Downloads\"]\n",
    "        d = next((p for p in candidates if p.exists()), Path.cwd()/ \"grading_reports\")\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def _make_pdf(output_path, context):\n",
    "    \"\"\"\n",
    "    PDF with wrapped table cells using Paragraphs.\n",
    "    context expects:\n",
    "      \"timestamp\",\"candidate_model\",\"grader_model\",\n",
    "      \"prompt_name\",\"rubric_name\",\"ref_names\",\n",
    "      \"candidate_answer\",\"items_df\",\"signed_total\",\"normalized\"\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    styles.add(ParagraphStyle(\n",
    "        name=\"Cell\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontSize=9,\n",
    "        leading=11,\n",
    "        splitLongWords=True,\n",
    "    ))\n",
    "    mono = ParagraphStyle(\n",
    "        name=\"Mono\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontName=\"Courier\",\n",
    "        fontSize=9,\n",
    "        leading=10,\n",
    "    )\n",
    "\n",
    "    doc = SimpleDocTemplate(\n",
    "        str(output_path), pagesize=letter,\n",
    "        rightMargin=36, leftMargin=36, topMargin=36, bottomMargin=36\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    # Title & meta\n",
    "    story.append(Paragraph(\"Grading Report\", styles[\"Title\"]))\n",
    "    story.append(Spacer(1, 6))\n",
    "    meta_lines = [\n",
    "        f\"Generated: {context['timestamp']}\",\n",
    "        f\"Candidate model: {context['candidate_model']}\",\n",
    "        f\"Grader model: {context['grader_model']}\",\n",
    "        f\"Prompt file: {context.get('prompt_name') or '—'}\",\n",
    "        f\"Rubric file: {context.get('rubric_name') or '—'}\",\n",
    "    ]\n",
    "    if context.get(\"ref_names\"):\n",
    "        meta_lines.append(\"Attachments: \" + \", \".join(context[\"ref_names\"]))\n",
    "    story.append(Paragraph(\"<br/>\".join(meta_lines), styles[\"Normal\"]))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Candidate answer\n",
    "    story.append(Paragraph(\"Sample Response\", styles[\"Heading2\"]))\n",
    "    story.append(Spacer(1, 4))\n",
    "    story.append(Preformatted(context.get(\"candidate_answer\") or \"—\", mono))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Per-criterion table\n",
    "    story.append(Paragraph(\"Per-Criterion Decisions\", styles[\"Heading2\"]))\n",
    "    story.append(Spacer(1, 6))\n",
    "\n",
    "    df = context[\"items_df\"].copy()\n",
    "\n",
    "    def P(x):\n",
    "        return Paragraph(escape(str(x)).replace(\"\\n\", \"<br/>\"), styles[\"Cell\"])\n",
    "\n",
    "    headers = [\"#\", \"Criterion\", \"Score\", \"Verdict\", \"Awarded\", \"Rationale\"]\n",
    "    table_data = [headers]\n",
    "    for _, r in df.iterrows():\n",
    "        table_data.append([\n",
    "            str(r[\"#\"]),\n",
    "            P(r[\"criterion\"]),\n",
    "            f\"{float(r['score']):g}\",\n",
    "            str(r[\"verdict\"]),\n",
    "            f\"{float(r['awarded']):g}\",\n",
    "            P(r[\"rationale\"]),\n",
    "        ])\n",
    "\n",
    "    available = SimpleDocTemplate(str(output_path)).width  # same margins\n",
    "    fractions = [0.07, 0.34, 0.09, 0.12, 0.11, 0.27]\n",
    "    scale = min(1.0, 1.0 / sum(fractions))\n",
    "    col_widths = [doc.width * f * scale for f in fractions]\n",
    "\n",
    "    tbl = Table(table_data, colWidths=col_widths, repeatRows=1)\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"FONT\", (0,0), (-1,0), \"Helvetica-Bold\", 10),\n",
    "        (\"BACKGROUND\", (0,0), (-1,0), colors.lightgrey),\n",
    "        (\"LINEABOVE\", (0,0), (-1,0), 0.5, colors.black),\n",
    "        (\"LINEBELOW\", (0,0), (-1,0), 0.5, colors.black),\n",
    "        (\"GRID\", (0,1), (-1,-1), 0.25, colors.grey),\n",
    "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0,0), (-1,-1), 4),\n",
    "        (\"RIGHTPADDING\", (0,0), (-1,-1), 4),\n",
    "        (\"WORDWRAP\", (0,0), (-1,-1), True),\n",
    "    ]))\n",
    "    story.append(tbl)\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    signed = float(context[\"signed_total\"])\n",
    "    norm = float(context[\"normalized\"])\n",
    "    story.append(Paragraph(\"Totals\", styles[\"Heading2\"]))\n",
    "    story.append(Paragraph(f\"Total (signed): <b>{signed:g}</b>\", styles[\"Normal\"]))\n",
    "    story.append(Paragraph(f\"Normalized (0..1): <b>{norm:.3f}</b>\", styles[\"Normal\"]))\n",
    "\n",
    "    doc.build(story)\n",
    "    return str(output_path)\n",
    "\n",
    "# ---------- Main pipelines ----------\n",
    "def run_pipeline(prompt_file, rubric_file, ref_files,\n",
    "                 candidate_model, grader_model,\n",
    "                 api_key_text, exact_weight, rubric_weight):\n",
    "    \"\"\"\n",
    "    Full generate+grade path.\n",
    "    Saves session state so we can regrade later.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    def log(x): logs.append(str(x))\n",
    "    try:\n",
    "        cli = _ensure_client_from_key(api_key_text); log(\"OpenAI client ready.\")\n",
    "\n",
    "        if not prompt_file or not rubric_file:\n",
    "            return (None, None, \"\", pd.DataFrame(), None, \"\\n\".join(logs + [\"Please upload both a prompt and a rubric.\"]),\n",
    "                    \"\", \"\", [], \"\")  # <- states\n",
    "\n",
    "        prompt_path = _fpath(prompt_file)\n",
    "        rubric_path = _fpath(rubric_file)\n",
    "        prompt_text = _read_text_file(prompt_path)\n",
    "        rubric_raw = _read_json_file(rubric_path)\n",
    "        if not isinstance(rubric_raw, list) or not all((\"criterion\" in x and \"score\" in x) for x in rubric_raw):\n",
    "            return (None, None, \"\", pd.DataFrame(), None, \"\\n\".join(logs + [\"Rubric must be a JSON list of {criterion, score}.\"]),\n",
    "                    \"\", \"\", [], \"\")\n",
    "\n",
    "        prompt_name = Path(prompt_path).name if prompt_path else \"\"\n",
    "        rubric_name = Path(rubric_path).name if rubric_path else \"\"\n",
    "        ref_names = [Path(_fpath(r)).name for r in (ref_files or []) if _fpath(r)]\n",
    "\n",
    "        log(f\"Loaded prompt ({len(prompt_text)} chars) and rubric ({len(rubric_raw)} items).\")\n",
    "\n",
    "        # Attachments (optional) for candidate generation\n",
    "        vs_id = _create_vs_and_upload_refs(cli, ref_files); log(f\"Vector store ready: {vs_id}\")\n",
    "        log(\"Attachments processed (unsupported types auto-converted to .txt for retrieval).\")\n",
    "\n",
    "\n",
    "        # Candidate generation\n",
    "        candidate_answer = generate_candidate_answer(prompt_text, candidate_model=candidate_model); log(\"Candidate answer generated.\")\n",
    "\n",
    "        # Grading\n",
    "        sample = {\"prompt\": prompt_text, \"reference\": \"\"}\n",
    "        final_norm, details = multigrader(\n",
    "            sample, candidate_answer, rubric_raw,\n",
    "            grader_model=grader_model,\n",
    "            exact_weight=float(exact_weight), rubric_weight=float(rubric_weight),\n",
    "        )\n",
    "        log(\"Grading complete.\")\n",
    "\n",
    "        items_raw = (details or {}).get(\"rubric_details\", {}).get(\"items\", [])\n",
    "        items_df = _build_items_table(items_raw)\n",
    "        signed_total = float((details or {}).get(\"rubric_total\", 0.0))\n",
    "        final_norm = float(final_norm)\n",
    "\n",
    "        # PDF path (Downloads by default if present)\n",
    "        out_dir = _out_dir()\n",
    "        ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        pdf_path = out_dir / f\"grading_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "        _make_pdf(pdf_path, {\n",
    "            \"timestamp\": ts,\n",
    "            \"candidate_model\": candidate_model,\n",
    "            \"grader_model\": grader_model,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"rubric_name\": rubric_name,\n",
    "            \"ref_names\": ref_names,\n",
    "            \"candidate_answer\": candidate_answer,\n",
    "            \"items_df\": items_df,\n",
    "            \"signed_total\": signed_total,\n",
    "            \"normalized\": final_norm,\n",
    "        })\n",
    "        log(f\"PDF created: {pdf_path}\")\n",
    "\n",
    "        # Return outputs + session state for regrading\n",
    "        return (final_norm, signed_total, candidate_answer, items_df, str(pdf_path), \"\\n\".join(logs),\n",
    "                prompt_text, prompt_name, ref_names, candidate_answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        logs.append(f\"ERROR: {e}\\n{tb}\")\n",
    "        return (None, None, \"\", pd.DataFrame(), None, \"\\n\".join(logs),\n",
    "                \"\", \"\", [], \"\")\n",
    "\n",
    "def regrade_pipeline(rubric_file, grader_model, api_key_text,\n",
    "                     exact_weight, rubric_weight,\n",
    "                     saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer,\n",
    "                     override_answer):\n",
    "    \"\"\"\n",
    "    Regrade the *existing* candidate answer (no regeneration).\n",
    "    - Uses the rubric file provided (you can swap it if you like).\n",
    "    - If `override_answer` is non-empty, that text is graded instead of the saved answer.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    def log(x): logs.append(str(x))\n",
    "    try:\n",
    "        _ensure_client_from_key(api_key_text); log(\"OpenAI client ready (regrade).\")\n",
    "\n",
    "        if not rubric_file:\n",
    "            return (None, None, saved_candidate_answer or \"\", pd.DataFrame(), None,\n",
    "                    \"\\n\".join(logs + [\"Please provide a rubric file to regrade.\"]),\n",
    "                    saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer)\n",
    "\n",
    "        if not (override_answer or saved_candidate_answer):\n",
    "            return (None, None, \"\", pd.DataFrame(), None,\n",
    "                    \"\\n\".join(logs + [\"No candidate answer available to regrade. Run a full generation first or paste an override.\"]),\n",
    "                    saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer)\n",
    "\n",
    "        rubric_path = _fpath(rubric_file)\n",
    "        rubric_raw = _read_json_file(rubric_path)\n",
    "        if not isinstance(rubric_raw, list) or not all((\"criterion\" in x and \"score\" in x) for x in rubric_raw):\n",
    "            return (None, None, saved_candidate_answer or \"\", pd.DataFrame(), None,\n",
    "                    \"\\n\".join(logs + [\"Rubric must be a JSON list of {criterion, score}.\"]),\n",
    "                    saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer)\n",
    "\n",
    "        rubric_name = Path(rubric_path).name if rubric_path else \"\"\n",
    "        candidate_answer = override_answer if (override_answer and override_answer.strip()) else saved_candidate_answer\n",
    "        prompt_text = saved_prompt_text or \"\"\n",
    "\n",
    "        # Grade (no generation)\n",
    "        sample = {\"prompt\": prompt_text, \"reference\": \"\"}\n",
    "        final_norm, details = multigrader(\n",
    "            sample, candidate_answer, rubric_raw,\n",
    "            grader_model=grader_model,\n",
    "            exact_weight=float(exact_weight), rubric_weight=float(rubric_weight),\n",
    "        )\n",
    "        log(\"Regrade complete.\")\n",
    "\n",
    "        items_raw = (details or {}).get(\"rubric_details\", {}).get(\"items\", [])\n",
    "        items_df = _build_items_table(items_raw)\n",
    "        signed_total = float((details or {}).get(\"rubric_total\", 0.0))\n",
    "        final_norm = float(final_norm)\n",
    "\n",
    "        # PDF (label as regrade)\n",
    "        out_dir = _out_dir()\n",
    "        ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        pdf_path = out_dir / f\"grading_regrade_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "        _make_pdf(pdf_path, {\n",
    "            \"timestamp\": ts,\n",
    "            \"candidate_model\": \"(regrade - unchanged)\",  # candidate model irrelevant on regrade\n",
    "            \"grader_model\": grader_model,\n",
    "            \"prompt_name\": saved_prompt_name,\n",
    "            \"rubric_name\": rubric_name,\n",
    "            \"ref_names\": saved_ref_names or [],\n",
    "            \"candidate_answer\": candidate_answer,\n",
    "            \"items_df\": items_df,\n",
    "            \"signed_total\": signed_total,\n",
    "            \"normalized\": final_norm,\n",
    "        })\n",
    "        log(f\"Regrade PDF created: {pdf_path}\")\n",
    "\n",
    "        # Persist the possibly overridden answer\n",
    "        new_saved_answer = candidate_answer\n",
    "\n",
    "        return (final_norm, signed_total, candidate_answer, items_df, str(pdf_path), \"\\n\".join(logs),\n",
    "                saved_prompt_text, saved_prompt_name, saved_ref_names, new_saved_answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        logs.append(f\"ERROR: {e}\\n{tb}\")\n",
    "        return (None, None, saved_candidate_answer or \"\", pd.DataFrame(), None, \"\\n\".join(logs),\n",
    "                saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer)\n",
    "\n",
    "# ---------- Build the Gradio app ----------\n",
    "default_candidate_models = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4.1\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "    \"o3-mini-2025-01-31\",\n",
    "]\n",
    "default_grader_models = [\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(title=\"Rubric Grader\") as demo:\n",
    "    gr.Markdown(\"## Rubric Grader\\nUpload your **prompt** (.txt/.md), **rubric** (.json), and any **attachments**. Choose models and click **Generate & Grade**. Then you can **Regrade** the same answer with a different grader model.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt_file = gr.File(label=\"Prompt (.txt / .md)\", file_types=[\".txt\", \".md\"], type=\"filepath\")\n",
    "        rubric_file = gr.File(label=\"Rubric (.json)\", file_types=[\".json\"], type=\"filepath\")\n",
    "        ref_files   = gr.Files(label=\"Attachments for generation (optional)\", file_count=\"multiple\")\n",
    "\n",
    "    with gr.Row():\n",
    "        candidate_model = gr.Dropdown(default_candidate_models, label=\"Candidate model\", value=default_candidate_models[0], allow_custom_value=True)\n",
    "        grader_model    = gr.Dropdown(default_grader_models,    label=\"Grader model\",    value=default_grader_models[0], allow_custom_value=True)\n",
    "\n",
    "    with gr.Accordion(\"Advanced / Auth\", open=False):\n",
    "        api_key_text = gr.Textbox(label=\"OpenAI API Key (optional if set in env)\", type=\"password\", placeholder=\"sk-...\")\n",
    "        with gr.Row():\n",
    "            exact_weight  = gr.Slider(0.0, 1.0, value=0.0, step=0.05, label=\"Exact-match weight\")\n",
    "            rubric_weight = gr.Slider(0.0, 1.0, value=1.0, step=0.05, label=\"Rubric weight\")\n",
    "\n",
    "    # Primary actions\n",
    "    with gr.Row():\n",
    "        run_btn = gr.Button(\"Generate & Grade\", variant=\"primary\")\n",
    "        regrade_btn = gr.Button(\"Regrade current answer (skip generation)\")\n",
    "\n",
    "    # Outputs\n",
    "    with gr.Row():\n",
    "        final_norm  = gr.Number(label=\"Final score (normalized 0..1)\")\n",
    "        total_signed = gr.Number(label=\"Total score (signed)\")\n",
    "    sample_response = gr.Textbox(label=\"Sample response\", lines=12, interactive=False)\n",
    "    items_table_out = gr.Dataframe(label=\"Per-criterion\", interactive=False, wrap=True)\n",
    "    pdf_out = gr.File(label=\"Download final deliverable (PDF)\")\n",
    "    logs_out = gr.Textbox(label=\"Run log\", lines=6)\n",
    "\n",
    "    # Optional override for regrading:\n",
    "    with gr.Accordion(\"Regrade options\", open=False):\n",
    "        override_answer = gr.Textbox(label=\"Override candidate answer (optional)\", lines=8, placeholder=\"Paste a candidate answer here to regrade it without regenerating.\")\n",
    "\n",
    "    # ---- Session state (hidden) ----\n",
    "    saved_prompt_text = gr.State(\"\")\n",
    "    saved_prompt_name = gr.State(\"\")\n",
    "    saved_ref_names   = gr.State([])\n",
    "    saved_candidate_answer = gr.State(\"\")\n",
    "\n",
    "    # Wiring: Generate & Grade\n",
    "    run_btn.click(\n",
    "        fn=run_pipeline,\n",
    "        inputs=[prompt_file, rubric_file, ref_files,\n",
    "                candidate_model, grader_model,\n",
    "                api_key_text, exact_weight, rubric_weight],\n",
    "        outputs=[final_norm, total_signed, sample_response, items_table_out, pdf_out, logs_out,\n",
    "                 saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer],\n",
    "    )\n",
    "\n",
    "    # Wiring: Regrade (no generation)\n",
    "    regrade_btn.click(\n",
    "        fn=regrade_pipeline,\n",
    "        inputs=[rubric_file, grader_model, api_key_text,\n",
    "                exact_weight, rubric_weight,\n",
    "                saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer,\n",
    "                override_answer],\n",
    "        outputs=[final_norm, total_signed, sample_response, items_table_out, pdf_out, logs_out,\n",
    "                 saved_prompt_text, saved_prompt_name, saved_ref_names, saved_candidate_answer],\n",
    "    )\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df803a70-f0aa-48ca-bb72-e5d2351f2448",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
